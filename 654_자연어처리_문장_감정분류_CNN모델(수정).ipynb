{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDxq5eLn/umDA9xrUxlehS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onsoon829/dataproject/blob/master/654_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_%EB%AC%B8%EC%9E%A5_%EA%B0%90%EC%A0%95%EB%B6%84%EB%A5%98_CNN%EB%AA%A8%EB%8D%B8(%EC%88%98%EC%A0%95).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp7gWe2jDN-g",
        "outputId": "4f710b72-ebb8-4325-f9c2-560c52dbf776"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/ai_chat_python\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd  '/content/drive/MyDrive/ai_chat_python'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# * 문장 영화리뷰 분류 CNN 모델*"
      ],
      "metadata": {
        "id": "7aKp9pG4Dy_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 모듈 임포트\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import preprocessing\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
        "import pickle\n",
        "\n",
        "# 데이터 읽어오기\n",
        "train_file = \"./datasets/chatbot_data.csv\"\n",
        "data = pd.read_csv(train_file, delimiter=',')\n",
        "features = data['Q'].tolist()\n",
        "labels = data['label'].tolist()\n",
        "\n",
        "print(data.head())\n",
        "print(data['label'].value_counts()) # 0일상다반사, 1:이별(부정), 2:사랑(긍정)\n",
        "\n",
        "# 단어 인덱스 시퀀스 벡터: 질문 리스트(features)에서 한 문장씩 꺼내와 text_to_word_sequesce()함수를 이용한다.\n",
        "# 만든 후 corpus변수에 저장한다.\n",
        "# 단어 인덱스 스퀀스 벡터\n",
        "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "# 텍스트 토큰화시키기\n",
        "# 가장 빈도수가 높은 1000개의 단으ㅓ만 선택하도록 하는 Tokenizer 객체\n",
        "tokenizer = preprocessing.text.Tokenizer(num_words=1000)\n",
        "print(dir(tokenizer))\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "print('===================')\n",
        "print(tokenizer)\n",
        "print(dir(tokenizer))\n",
        "\n",
        "with open('vocab.pickle', 'wb') as fp:\n",
        "  pickle.dump(tokenizer, fp)\n",
        "\n",
        "\n",
        "# 문장 내 모든 단어를 시퀀스 번호로 변환한다.\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "print('sequences:' , sequences[0:5])\n",
        "#print(sequences)\n",
        "word_index = tokenizer.word_index\n",
        "#print(word_index)\n",
        "\n",
        "MAX_SEQ_LEN = 15 # 단어 시퀀스 벡터 크기  pad: padding        시퀀스를 맥스로 해서 15개 채운다\n",
        "# 단어 시퀀스 벡터 크기, padding = 'pre'(0을 앞에 채움), padding='post' (0을 뒤에 채움)\n",
        "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
        "print(padded_seqs)\n",
        "print(padded_seqs[0]) # ?\n",
        "\n",
        "# 학습용, 검증용, 테스트용 데이터셋 생성.\n",
        "# 학습용, 검증용, 테스트용 = 7:2:1 요 비율대로 나눠준다.\n",
        "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
        "ds = ds.shuffle(len(features))\n",
        "\n",
        "train_size = int(len(padded_seqs) *0.7)\n",
        "val_size = int(len(padded_seqs)*0.2)\n",
        "test_size = int(len(padded_seqs)*0.1)\n",
        "# 20씩 처리한다는 얘기.\n",
        "train_ds = ds.take(train_size).batch(20)\n",
        "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
        "test_ds = ds.skip(train_size+val_size).take(test_size).batch(20)\n",
        "\n",
        "# 하이퍼파라미터\n",
        "dropout_prob=0.5\n",
        "EMB_SIZE=128\n",
        "EPOCH=5\n",
        "VOCAB_SIZE = len(word_index) + 1 # 전체 단어 수\n",
        "\n",
        "\n",
        "# CNN모델 정의\n",
        "input_layer = Input(shape=(MAX_SEQ_LEN))\n",
        "print('input_layer :', input_layer.shape)\n",
        "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
        "print('embedding_layer :', embedding_layer.shape)\n",
        "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
        "print('dropout_emb :', dropout_emb.shape)\n",
        "\n",
        "# 풀링 연산이란 합성곱 연산 결과로 나온 특징맵의 크기를 줄이거나 주요한 특징을 추출하기 위해서 사용되는 연산이다.\n",
        "# 풀링 연산에는 최대풀링과 평균풀링 연산이 있는데 주로 최대 풀링 연산을 사용한다.\n",
        "# 풀링 연산에도 합성곱연산에서 사용되는 윈도우 크기, 스트라이드, 패딩 개념등이 동일하게 사용된다.\n",
        "# 아래 코드들: 특징 추출\n",
        "conv1 = Conv1D(filters=128, kernel_size=3, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
        "print('conv1 :', conv1.shape)  # conv1: (None, 13, 128)\n",
        "pool1 = GlobalMaxPool1D()(conv1)\n",
        "print('pool1 :', pool1.shape)  # pool1: (None, 128)\n",
        "conv2 = Conv1D(filters=128, kernel_size=4, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
        "print('conv2 :', conv2.shape)  # conv2: (None, 12, 128)\n",
        "pool2 = GlobalMaxPool1D()(conv2)\n",
        "print('pool2 :', pool2.shape)  # pool2: (None, 128)\n",
        "conv3 = Conv1D(filters=128, kernel_size=5, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
        "print('conv3 :', conv3.shape)  # conv3: (None, 11, 128)\n",
        "pool3 = GlobalMaxPool1D()(conv3)\n",
        "print('pool3 :', pool2.shape)  # pool3: (None, 128)\n",
        "\n",
        "# 3, 4, 5-gram 이후 합치기\n",
        "concat = concatenate([pool1, pool2, pool3])\n",
        "\n",
        "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
        "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
        "logits = Dense(3, name='logits')(dropout_hidden)\n",
        "predictions = Dense(3, activation=tf.nn.softmax)(logits)\n",
        "\n",
        "# 모델생성\n",
        "model = Model(inputs=input_layer, outputs=predictions)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) # 경사하강법 어떻게 적용할지 고민\n",
        "\n",
        "# 모델학습\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH)\n",
        "\n",
        "# 모델평가(테스트 데이터셋 이용)\n",
        "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
        "print('Accuracy: %f' % (accuracy * 100))\n",
        "print('loss:%f' %(loss))\n",
        "\n",
        "# 모델 저장\n",
        "model.save('./source/cnn_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcqrw4qHD7Er",
        "outputId": "fb25279d-8502-402c-a5d1-48829633477c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Q            A  label\n",
            "0           12시 땡!   하루가 또 가네요.      0\n",
            "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
            "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
            "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
            "4          PPL 심하네   눈살이 찌푸려지죠.      0\n",
            "0    5290\n",
            "1    3570\n",
            "2    2963\n",
            "Name: label, dtype: int64\n",
            "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__subclasshook__', '__weakref__', '_keras_api_names', '_keras_api_names_v1', 'analyzer', 'char_level', 'document_count', 'filters', 'fit_on_sequences', 'fit_on_texts', 'get_config', 'index_docs', 'index_word', 'lower', 'num_words', 'oov_token', 'sequences_to_matrix', 'sequences_to_texts', 'sequences_to_texts_generator', 'split', 'texts_to_matrix', 'texts_to_sequences', 'texts_to_sequences_generator', 'to_json', 'word_counts', 'word_docs', 'word_index']\n",
            "===================\n",
            "<keras.src.preprocessing.text.Tokenizer object at 0x7f0c032e4a60>\n",
            "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__subclasshook__', '__weakref__', '_keras_api_names', '_keras_api_names_v1', 'analyzer', 'char_level', 'document_count', 'filters', 'fit_on_sequences', 'fit_on_texts', 'get_config', 'index_docs', 'index_word', 'lower', 'num_words', 'oov_token', 'sequences_to_matrix', 'sequences_to_texts', 'sequences_to_texts_generator', 'split', 'texts_to_matrix', 'texts_to_sequences', 'texts_to_sequences_generator', 'to_json', 'word_counts', 'word_docs', 'word_index']\n",
            "sequences: [[], [343, 448], [803, 11], [804, 803, 11], []]\n",
            "[[  0   0   0 ...   0   0   0]\n",
            " [343 448   0 ...   0   0   0]\n",
            " [803  11   0 ...   0   0   0]\n",
            " ...\n",
            " [ 89   0   0 ...   0   0   0]\n",
            " [147  46  91 ...   0   0   0]\n",
            " [555   0   0 ...   0   0   0]]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "input_layer : (None, 15)\n",
            "embedding_layer : (None, 15, 128)\n",
            "dropout_emb : (None, 15, 128)\n",
            "conv1 : (None, 13, 128)\n",
            "pool1 : (None, 128)\n",
            "conv2 : (None, 12, 128)\n",
            "pool2 : (None, 128)\n",
            "conv3 : (None, 11, 128)\n",
            "pool3 : (None, 128)\n",
            "Epoch 1/5\n",
            "414/414 [==============================] - 9s 18ms/step - loss: 0.9026 - accuracy: 0.5532 - val_loss: 0.6737 - val_accuracy: 0.7026\n",
            "Epoch 2/5\n",
            "414/414 [==============================] - 8s 20ms/step - loss: 0.6482 - accuracy: 0.7244 - val_loss: 0.5777 - val_accuracy: 0.7479\n",
            "Epoch 3/5\n",
            "414/414 [==============================] - 8s 20ms/step - loss: 0.5811 - accuracy: 0.7529 - val_loss: 0.5315 - val_accuracy: 0.7686\n",
            "Epoch 4/5\n",
            "414/414 [==============================] - 8s 19ms/step - loss: 0.5521 - accuracy: 0.7540 - val_loss: 0.4869 - val_accuracy: 0.7830\n",
            "Epoch 5/5\n",
            "414/414 [==============================] - 8s 19ms/step - loss: 0.5368 - accuracy: 0.7608 - val_loss: 0.4610 - val_accuracy: 0.7931\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.4854 - accuracy: 0.7851\n",
            "Accuracy: 78.511000\n",
            "loss:0.485357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# * 문답 데이터 감정분류 테스트 *"
      ],
      "metadata": {
        "id": "_9s0I2y_JIoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from keras.models import Model, load_model\n",
        "from keras import preprocessing\n",
        "import pickle\n",
        "\n",
        "# 데이터 읽어오기\n",
        "train_file = \"./datasets/chatbot_data.csv\"\n",
        "data = pd.read_csv(train_file, delimiter=',')\n",
        "features = data['Q'].tolist()\n",
        "labels = data['label'].tolist()\n",
        "\n",
        "# 단어 인덱스 시퀀스 벡터 워드로 인덱스 생성해야 한다.\n",
        "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
        "#tokenizer = preprocessing.text.Tokenizer()\n",
        "#tokenizer.fit_on_texts(corpus)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# train에서 파일에 저장해놓은 vocab을 읽어온다.\n",
        "with open('vocab.pickle', 'rb') as fp:\n",
        "  tokenizer = pickle.load(fp)\n",
        "\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "MAX_SEQ_LEN = 15 # 단어 시퀀스 벡터 크기\n",
        "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
        "\n",
        "# 테스트용 데이터셋 생성\n",
        "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
        "ds = ds.shuffle(len(features))\n",
        "test_ds = ds.take(2000).batch(20) # 테스트 데이터셋\n",
        "# 감정 분류 CNN 모델 불러오기\n",
        "model = load_model('./datasets/cnn_model.h5') # 있는 파일도 없다고 뻥치면 어떡하니ㄷ?\n",
        "model.summary()\n",
        "model.evaluate(test_ds, verbose=2)\n",
        "\n",
        "# 테스트용 데이터셋의 10212번째 데이터 출력\n",
        "print(\"단어 시퀀스 : \", corpus[10212])\n",
        "print(\"단어 인덱스 시퀀스 : \", padded_seqs[10212])\n",
        "print(\"문장 분류(정답) : \", labels[10212])\n",
        "\n",
        "# 테스트용 데이터셋의 10212번째 데이터 감정 예측\n",
        "picks = [10212]\n",
        "predict = model.predict(padded_seqs[picks])\n",
        "predict_class = tf.math.argmax(predict, axis=1)\n",
        "print(\"감정 예측 점수 : \", predict)\n",
        "print(\"감정 예측 클래스 : \", predict_class.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udwmSQYxJDAY",
        "outputId": "d5673d98-47e3-46c0-b4e2-9c62f3b0832c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 15)]                 0         []                            \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, 15, 128)              1715072   ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 15, 128)              0         ['embedding_1[0][0]']         \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)           (None, 13, 128)              49280     ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)           (None, 12, 128)              65664     ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)           (None, 11, 128)              82048     ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_3 (Gl  (None, 128)                  0         ['conv1d_3[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " global_max_pooling1d_4 (Gl  (None, 128)                  0         ['conv1d_4[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " global_max_pooling1d_5 (Gl  (None, 128)                  0         ['conv1d_5[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 384)                  0         ['global_max_pooling1d_3[0][0]\n",
            " )                                                                  ',                            \n",
            "                                                                     'global_max_pooling1d_4[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'global_max_pooling1d_5[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 128)                  49280     ['concatenate_1[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 128)                  0         ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " logits (Dense)              (None, 3)                    387       ['dropout_3[0][0]']           \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 3)                    12        ['logits[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1961743 (7.48 MB)\n",
            "Trainable params: 1961743 (7.48 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "100/100 - 1s - loss: 0.6516 - accuracy: 0.7300 - 507ms/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0c039a0700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 시퀀스 :  ['썸', '타는', '여자가', '남사친', '만나러', '간다는데', '뭐라', '해']\n",
            "단어 인덱스 시퀀스 :  [ 13  61 127 856  31   0   0   0   0   0   0   0   0   0   0]\n",
            "문장 분류(정답) :  2\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "감정 예측 점수 :  [[1.0889631e-04 3.6781712e-04 9.9952328e-01]]\n",
            "감정 예측 클래스 :  [2]\n"
          ]
        }
      ]
    }
  ]
}