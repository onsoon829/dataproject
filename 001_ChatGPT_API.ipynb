{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGfV7679DzJCauyLmCbQyr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onsoon829/dataproject/blob/master/001_ChatGPT_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[';'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd  '/content/drive/MyDrive/ai_chat_python'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vtzf53Mx1GN",
        "outputId": "d61e3707-c732-49db-c44d-7a654e20ca30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/ai_chat_python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3qyDYt9xnhS",
        "outputId": "e803e012-3114-4759-e96b-91314224ff63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28.1\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m61.4/77.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n"
          ]
        }
      ],
      "source": [
        "# openai 라이브러리 설치\n",
        "!pip install openai==0.28.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "YCpT0sv2yDxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uevV8D-SyGc0",
        "outputId": "51287749-f5cd-4c79-e39c-5a51776bcc87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.28.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "질문하기 위한 기본 형식\n",
        " - model: 사용할 언어 모델 지정\n",
        " - message: 사용자가 입력할 프롬프트가 포하모딘 리스트이다.\n",
        " - temperature: 텍스트의 랜덤성 제어하는 파라미터. 지수가 높을수록 포괄적으로 답함, 지수가 낮을수록 사용자의 질문에 초점을 맞춰서 대답.\n",
        " - top_p(핵 샘플링): 텍스트의 랜덤성 제어하는 파라미터.\n",
        " - presence penalty: 단어가 이미 생성된 텍스트에 나타난 경우 해당 단어가 등장할 가능성 줄인다.\n",
        " - frequency penalty: 모델이 동일한 단어를 반복적으로 생성하지 않도록 설정하는 값\n",
        " - n(응답개수): 입력 메세지에 대해 생성할 답변 수 설정\n",
        " - max tokens: 토큰 최대 생성 제한.\n",
        " - stop: 토큰 생성 중지"
      ],
      "metadata": {
        "id": "o9jQ9h3mCPxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - model: 'gpt-3.5-turbo', // 사용할 모델\n",
        " - messages: messages, // 사용될 메시지\n",
        " - temperature: 1, // 샘플링값\n",
        " - top_p: 1, // 샘플링시 상위값을 가져올 확률\n",
        " - n: 1, // 답변을 몇개 가져올지\n",
        " - stream: false, // 스트림형식으로 response를 전달할지 여부\n",
        " - stop: undefined, // api가 답변을 중지할 시점 string 또는 배열\n",
        " - max_tokens: 300, // 토큰 최대량\n",
        " - presence_penalty: 0, // 모델이 기존 텍스트를 참고하여 새로운 대답을 하는 빈도수\n",
        " - frequency_penalty: 0, // 모델이 기존 텍스트를 참고하여 똑같은 대답을 하는 빈도수\n",
        " - logit_bias: null, // 모델이 특정 토큰을 답변에 포함할지 말지 여부 객체로 전달된다\n",
        " - user: undefined, // api 요청시 사용자 아이디"
      ],
      "metadata": {
        "id": "C0oZTlN5mz-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 기본 질문하기"
      ],
      "metadata": {
        "id": "uxcPIqROyMXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key='' # openAPI API key\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model='gpt-3.5.turbo',\n",
        "    messages=[{\"role\"}]\n",
        ")"
      ],
      "metadata": {
        "id": "PHqZWyVNyPVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 코드로 형식 지정됨\n",
        "import openai\n",
        "\n",
        "openai.api_key=''  # openAPI API key\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[{\"role\": \"user\", \"content\": \"프롬프트 입력\"}],\n",
        "  temperature=1,\n",
        "  top_p=1,\n",
        "  presence_penalty=1,\n",
        "  frequency_penalty=1,\n",
        "  n=1,\n",
        "  max_tokens=4000,\n",
        "  stop=None\n",
        ")\n",
        "\n",
        "print(response)\n",
        "\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joEoCVFdyx6O",
        "outputId": "ada0faf0-646e-43e6-a89e-cf3c3985ef74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-8aH4J5U1FKQFiEzrkjTVlOBfjzXot\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1703656111,\n",
            "  \"model\": \"gpt-3.5-turbo-0613\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"\\uc548\\ub155\\ud558\\uc138\\uc694! \\ud504\\ub86c\\ud504\\ud2b8 \\uc785\\ub825\\uc774\\ub780 \\uc0ac\\uc6a9\\uc790\\ub85c\\ubd80\\ud130 \\uc9c1\\uc811 \\uac12\\uc744 \\uc785\\ub825 \\ubc1b\\ub294 \\uac83\\uc744 \\ub9d0\\ud569\\ub2c8\\ub2e4.\\n\\n\\uc608\\ub97c \\ub4e4\\uc5b4, \\ub2e4\\uc74c\\uacfc \\uac19\\uc740 \\ucf54\\ub4dc\\uac00 \\uc788\\ub2e4\\uace0 \\ud574\\ubcf4\\uaca0\\uc2b5\\ub2c8\\ub2e4.\\n```\\nlet name = prompt(\\\"\\uc774\\ub984\\uc744 \\uc785\\ub825\\ud574\\uc8fc\\uc138\\uc694: \\\");\\nconsole.log(`\\uc548\\ub155\\ud558\\uc138\\uc694, ${name}\\ub2d8!`);\\n```\\n\\n\\uc704\\uc758 \\ucf54\\ub4dc\\uc5d0\\uc11c `prompt` \\ud568\\uc218\\ub97c \\ud1b5\\ud574 \\uc0ac\\uc6a9\\uc790\\ub85c\\ubd80\\ud130 \\uc774\\ub984\\uc744 \\uc785\\ub825 \\ubc1b\\uc544\\uc11c \\ubcc0\\uc218 `name`\\uc5d0 \\ud560\\ub2f9\\ud55c \\ub4a4, \\ucf58\\uc194\\uc5d0 \\uc778\\uc0ac\\ub9d0\\uc744 \\ucd9c\\ub825\\ud558\\ub294 \\uac83\\uc785\\ub2c8\\ub2e4. \\uc2e4\\ud589\\ud558\\uba74 \\\"\\uc774\\ub984\\uc744 \\uc785\\ub825\\ud574\\uc8fc\\uc138\\uc694: \\\"\\ub77c\\ub294 \\uba54\\uc2dc\\uc9c0\\uac00 \\ub098\\uc624\\uace0, \\uc0ac\\uc6a9\\uc790\\uac00 \\uc774\\ub984\\uc744 \\uc785\\ub825\\ud560 \\uc218 \\uc788\\uac8c \\ub429\\ub2c8\\ub2e4.\\n\\n\\ud504\\ub86c\\ud504\\ud2b8\\ub294 \\uc8fc\\ub85c \\uc6f9 \\ube0c\\ub77c\\uc6b0\\uc800\\uc5d0\\uc11c \\uc790\\uc8fc \\uc0ac\\uc6a9\\ub418\\uba70, \\ube44\\uc2b7\\ud55c \\uae30\\ub2a5\\uc73c\\ub85c Node.js\\uc5d0\\uc11c\\ub3c4 `readline` \\ubaa8\\ub4c8 \\ub4f1\\uc744 \\ud65c\\uc6a9\\ud558\\uc5ec \\ud504\\ub86c\\ud504\\ud2b8\\uc640 \\uc720\\uc0ac\\ud55c \\ub3d9\\uc791\\uc744 \\uad6c\\ud604\\ud560 \\uc218 \\uc788\\uc2b5\\ub2c8\\ub2e4.\"\n",
            "      },\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 15,\n",
            "    \"completion_tokens\": 250,\n",
            "    \"total_tokens\": 265\n",
            "  },\n",
            "  \"system_fingerprint\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key=''\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[{\"role\": \"user\", \"content\": \"Tell me how to make a pizza\"}]\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NZyVpbB0Yml",
        "outputId": "c1799d89-7dac-401f-dcb7-0e80ebb55524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-8aH51MevS5ypZDFSZ5VJUV6QyIy7e\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1703656155,\n",
            "  \"model\": \"gpt-3.5-turbo-0613\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"Making a pizza involves several steps, including preparing the dough, creating the sauce, assembling the toppings, and baking it. Here's a general recipe to guide you through the process:\\n\\nIngredients:\\n- 2 \\u00bc cups all-purpose flour\\n- 1 teaspoon sugar\\n- 1 packet (2 \\u00bc teaspoons) active dry yeast\\n- 1 teaspoon salt\\n- 1 cup warm water (around 110\\u00b0F or 43\\u00b0C)\\n- 2 tablespoons olive oil\\n\\nFor the sauce:\\n- 1 can (400g) crushed tomatoes\\n- 1 tablespoon tomato paste\\n- 1 clove garlic, minced\\n- 1 teaspoon dried oregano\\n- \\u00bd teaspoon dried basil\\n- Salt and pepper to taste\\n- Optional: red pepper flakes for heat\\n\\nToppings:\\n- 2 cups shredded mozzarella cheese\\n- Any desired toppings like pepperoni, sliced vegetables, cooked sausage, etc.\\n- Optional: grated Parmesan cheese, fresh basil or other herbs for garnish\\n\\nInstructions:\\n\\n1. In a large mixing bowl, combine the flour, sugar, yeast, and salt. Gradually add the warm water and olive oil. Mix everything together until a dough forms.\\n2. Transfer the dough to a floured surface and knead it for about 5 minutes until it becomes smooth and elastic. If it's too sticky, you can add a bit more flour.\\n3. Place the dough in a greased bowl, cover it with a clean towel or plastic wrap, and let it rise for about 1-2 hours in a warm area until it doubles in size.\\n4. While the dough is rising, you can prepare the sauce. In a saucepan, combine the crushed tomatoes, tomato paste, minced garlic, dried oregano, dried basil, salt, pepper, and any additional seasonings you desire. Simmer the sauce on low heat for about 15-20 minutes to allow the flavors to meld.\\n5. Preheat your oven to 475\\u00b0F (245\\u00b0C) and place a baking stone or baking sheet inside to heat up.\\n6. Once the dough has risen, punch it down to release any air bubbles, then transfer it to a floured surface. Roll it out to your desired thickness and shape.\\n7. Carefully transfer the rolled-out dough onto a sheet of parchment paper (to make it easier to slide onto the hot baking stone or sheet later).\\n8. Spread the sauce evenly over the dough, leaving a small border around the edges. Sprinkle the shredded mozzarella cheese over the sauce, and add your chosen toppings.\\n9. Slide the pizza (along with the parchment paper) onto the preheated baking stone or sheet. Bake in the preheated oven for approximately 12-15 minutes, or until the crust is golden brown and the cheese is melted and bubbly.\\n10. Carefully remove the pizza from the oven using oven mitts or a pizza peel. Allow it to cool for a few minutes before slicing. Optionally, sprinkle some grated Parmesan cheese and fresh basil or herbs on top for added flavor.\\n\\nNow, enjoy your homemade pizza! Feel free to modify the recipe by adding your favorite ingredients or experimenting with various toppings to suit your taste.\"\n",
            "      },\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 14,\n",
            "    \"completion_tokens\": 657,\n",
            "    \"total_tokens\": 671\n",
            "  },\n",
            "  \"system_fingerprint\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[각 정보의 의미]\n",
        "- choices : 완료 개체 목록이다. 질문시 응답 개수(n)를 1로 설정하면 한개, 2로 설정하면 2개의 완료 개체가 리스트 형태로 저장된다.\n",
        "- index : 완료 개체의 인덱스이다.\n",
        "- message : 모델에서 생성된 메시지 내용이다. 'content'는 답변 내용 'role'은 질문 시 지정한 역할이다.\n",
        "- created : 요청한 시점의 타임스탬프이다.\n",
        "- object : 반환된 객체의 유형이다. ChatGPT의 경우 chat.completion 객체로 반환된다.\n",
        "- usage : 질문할 때 사용된 토큰 수, 응답할 때 사용한 토큰 수, 총 사용한 토큰 수를 각각 제공한다."
      ],
      "metadata": {
        "id": "KBeowbNu0mMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgqNgmom0opR",
        "outputId": "9994fc12-5ea2-4cef-94cc-8e26f0f88f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making a pizza involves several steps, including preparing the dough, creating the sauce, assembling the toppings, and baking it. Here's a general recipe to guide you through the process:\n",
            "\n",
            "Ingredients:\n",
            "- 2 ¼ cups all-purpose flour\n",
            "- 1 teaspoon sugar\n",
            "- 1 packet (2 ¼ teaspoons) active dry yeast\n",
            "- 1 teaspoon salt\n",
            "- 1 cup warm water (around 110°F or 43°C)\n",
            "- 2 tablespoons olive oil\n",
            "\n",
            "For the sauce:\n",
            "- 1 can (400g) crushed tomatoes\n",
            "- 1 tablespoon tomato paste\n",
            "- 1 clove garlic, minced\n",
            "- 1 teaspoon dried oregano\n",
            "- ½ teaspoon dried basil\n",
            "- Salt and pepper to taste\n",
            "- Optional: red pepper flakes for heat\n",
            "\n",
            "Toppings:\n",
            "- 2 cups shredded mozzarella cheese\n",
            "- Any desired toppings like pepperoni, sliced vegetables, cooked sausage, etc.\n",
            "- Optional: grated Parmesan cheese, fresh basil or other herbs for garnish\n",
            "\n",
            "Instructions:\n",
            "\n",
            "1. In a large mixing bowl, combine the flour, sugar, yeast, and salt. Gradually add the warm water and olive oil. Mix everything together until a dough forms.\n",
            "2. Transfer the dough to a floured surface and knead it for about 5 minutes until it becomes smooth and elastic. If it's too sticky, you can add a bit more flour.\n",
            "3. Place the dough in a greased bowl, cover it with a clean towel or plastic wrap, and let it rise for about 1-2 hours in a warm area until it doubles in size.\n",
            "4. While the dough is rising, you can prepare the sauce. In a saucepan, combine the crushed tomatoes, tomato paste, minced garlic, dried oregano, dried basil, salt, pepper, and any additional seasonings you desire. Simmer the sauce on low heat for about 15-20 minutes to allow the flavors to meld.\n",
            "5. Preheat your oven to 475°F (245°C) and place a baking stone or baking sheet inside to heat up.\n",
            "6. Once the dough has risen, punch it down to release any air bubbles, then transfer it to a floured surface. Roll it out to your desired thickness and shape.\n",
            "7. Carefully transfer the rolled-out dough onto a sheet of parchment paper (to make it easier to slide onto the hot baking stone or sheet later).\n",
            "8. Spread the sauce evenly over the dough, leaving a small border around the edges. Sprinkle the shredded mozzarella cheese over the sauce, and add your chosen toppings.\n",
            "9. Slide the pizza (along with the parchment paper) onto the preheated baking stone or sheet. Bake in the preheated oven for approximately 12-15 minutes, or until the crust is golden brown and the cheese is melted and bubbly.\n",
            "10. Carefully remove the pizza from the oven using oven mitts or a pizza peel. Allow it to cool for a few minutes before slicing. Optionally, sprinkle some grated Parmesan cheese and fresh basil or herbs on top for added flavor.\n",
            "\n",
            "Now, enjoy your homemade pizza! Feel free to modify the recipe by adding your favorite ingredients or experimenting with various toppings to suit your taste.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response['usage'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2uXFNZW1-I1",
        "outputId": "e11a564c-37c6-40d0-8a9f-cbcebea22c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"prompt_tokens\": 14,\n",
            "  \"completion_tokens\": 657,\n",
            "  \"total_tokens\": 671\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**총 비용 계산**\n",
        "\n",
        "gpt-3.5-turbo 모델을 사용했고, 해당 모델의 프롬프트 토큰은 1000 토큰당 $0.0015, 완료 토큰은 1000 토큰당 $0.002의 비용이 발생하므로 약 1.62120175원을 사용한다."
      ],
      "metadata": {
        "id": "i4HeKmQc2K7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key='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'\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[{\"role\": \"user\", \"content\": \"2002년 월드컵 어느나라가 우승했어?\"}]\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3Af3-o62E9T",
        "outputId": "19a17fc2-2610-4d83-fcb1-6f9ff8b48a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2002년 월드컵에서는 한국과 일본이 공동으로 개최를 하였고, 이 중 한국과 사이 좋지 않은 심판 점수에도 불구하고 탈락없이 조에서 1위를 차지하여 4강에 진출하였습니다. 그리고 결국 독일과 이탈리아를 이기고, 브라질과 결승에서 대결하였습니다. 하지만 우승은 브라질이 차지하였습니다. 그러므로 2002년 월드컵에서는 브라질이 우승하였습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 역할 부여하기\n",
        "\n",
        "- GhatGPT API를 이용해 ChatAPI를 사용할 때는 ChatGPT에게 역할을 지시할 수 있다.\n",
        "- 역할지시란 ChatGPT가 앞으로 답변할 때 해당 역할로서 답변이라는 의미이다.\n",
        "- 역할 지시 방법은 항상 지시한 역할대로 동작한다는 보장은 없지만, 역할 지시문에 따라 답변 자체의 방향성을 바꿔 버리기도 한다.\n",
        "- 역할을 지시하려면 기존 코드에서 {\"role\": \"system\", \"content\": \" \"}를 추가한다."
      ],
      "metadata": {
        "id": "J7KqkZML3r1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key=''\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[{\"role\": \"user\", \"content\": \"너는 초등학교 선생님이야, 초등학생들도 알 수 있게 설명해줘\"},\n",
        "            {\"role\": \"user\", \"content\": \"2020년 월드시리즈에서는 누가 우승했어?\"}\n",
        "            ],\n",
        "\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUBKhade-ECM",
        "outputId": "8d14d8e9-f2d8-456f-fd3f-f422e96490ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2020년 월드시리즈에서는 로스앤젤레스 다저스(Los Angeles Dodgers)가 우승했어요. 월드시리즈는 미국 메이저 리그 베이스볼(MLB)에서 진행되는 챔피언십 경기로, 미국과 캐나다에서 가장 강력한 2개의 리그인 앨리지 컵 챔피언십과 내셔널리그 플레이오프의 우승팀 사이에서 겨룹니다. 로스앤젤레스 다저스는 텍사스 주에 있는 글로브 라이프 파크에서 열린 월드시리즈에서 탬파베이 레이즈(Tampa Bay Rays)를 상대로 4승 2패로 승리하여 1988년 이후 32년 만에 월드시리즈 우승을 차지했습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGPT에게 친절하게 답변해 주는 비서라는 역할을 부여"
      ],
      "metadata": {
        "id": "-HUgK5lb3_rL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key=''\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[{\"role\": \"user\", \"content\": \"너는 친절하게 답변해주는 비서야\"},\n",
        "            {\"role\": \"user\", \"content\": \"2020년 월드시리즈에서는 누가 우승했어?\"}\n",
        "            ],\n",
        "\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzAdrT2J3qoN",
        "outputId": "55741ca0-0d20-4314-a159-9accfdc577c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2020년 월드시리즈에서는 로스앤젤레스 다저스가 탬파베이 레이스를 상대로 우승했어요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "챗gpt가 한글로 질문해도 영어로 답변하게끔 구현"
      ],
      "metadata": {
        "id": "jax9x_8Q8gbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key=''\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[{\"role\": \"user\", \"content\": \"You are a chatbot that answer questions in English even in Korean.\"},\n",
        "            {\"role\": \"user\", \"content\": \"2020년 월드시리즈에서는 누가 우승했어?\"}\n",
        "            ],\n",
        "\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oXUceq85xNs",
        "outputId": "b3570cbe-dc44-4dc6-f871-b70935c5360d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2020년 월드시리즈에서는 로스앤젤레스 다저스가 우승했습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "챗gpt가 답변을 거부하는 방식 구현"
      ],
      "metadata": {
        "id": "N5TFl89r9D3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key=''\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[{\"role\": \"user\", \"content\": \"You are a chatbot that refuses to answer and says sorry when users ask questions.\"},\n",
        "            {\"role\": \"user\", \"content\": \"2020년 월드시리즈에서는 누가 우승했어?\"}\n",
        "            ],\n",
        "\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NzPYpRL66Ww",
        "outputId": "06454275-ea19-440c-abca-6a72e410136d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, but I am unable to answer that question.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "챗gpt가 사용자의 질문을 영어로 번역하여 답변하는 지시문"
      ],
      "metadata": {
        "id": "MxupLWEM8Rxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key=''\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[{\"role\": \"user\", \"content\": \"You are a translator who translates user input.\"},\n",
        "            {\"role\": \"user\", \"content\": \"2020년 월드시리즈에서는 누가 우승했어?\"}\n",
        "            ],\n",
        "\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDy6Xrp38WXb",
        "outputId": "acc72b47-afaa-4c0b-cafd-f799712af25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who won the World Series in 2020?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZObpfh8F9Nbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 이전대화를 포함하여 답변하기\n",
        "\n",
        "- ChatGPT는 답변할 때 이전 질문과 답변을 모두 고려하여 답변하는 특징이 있다.\n",
        "- ChatGPT API를 이용하면 ChatGPT에게 답변을 요청할 때 '앞서 네가 이런 답변을 한 상태였다'라는 정보를 전달할 수 있다. 이것은 사용자가 가정하는 것이지만, ChatGPT는 마치 과거에 자신이 답변한 것으로 가정하고, 추가 답변을 제공한다.\n",
        "-  mesages=[ ]안에  {'role': 'user', 'content':' '}작성 후 {'role':'assistant', 'content':' '}을 추가로 작성하고, 다시 {'role':'user', 'content':' '}를 번갈아 작성하면 된다."
      ],
      "metadata": {
        "id": "teFmp33h9Ncz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key=''\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[{\"role\": \"user\", \"content\": \"2002년 월드컵에서 가장 화제가 되었던 나라는 어디야?\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"바로 예상을ㄹ 뚫고 4강 진출 신화를 일으킨 한국입니다.\"},\n",
        "            {\"role\": \"user\", \"content\": \"그 나라가 화제가 되었던 이유를 자세하게 설명해줘\"}\n",
        "            ],\n",
        "\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uE6ElWSf9OF6",
        "outputId": "a96e5fb7-1bf7-428f-eb64-20e67719c7c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한국이 2002년 월드컵에서 화제가 된 이유는 몇 가지가 있습니다.\n",
            "\n",
            "첫째, 예선을 통과해서 16강에 진출하는 것 자체가 큰 충격이었습니다. 한국은 그룹 리그에서 이탈리아, 포르투갈, 폴란드와 맞붙었는데, 이들을 상대로 무승부를 거두고 4강에 올라갔습니다. 특히 이탈리아와의 경기에서는 논란이 되는 심판 판정이 있었는데, 이를 통해 한국은 큰 화제가 되었습니다.\n",
            "\n",
            "둘째, 한국에서 개최된 월드컵이었기 때문에, 한국에서의 관심도 크고 열기도 더 뜨거웠습니다. 한국은 열정적인 축구 팬들로 유명하고, 그 열기는 월드컵에서 특히 돋보였습니다. 한국에서의 월드컵 분위기와 축구 문화는 전 세계적으로 주목받았고, 많은 경기를 관람하러 온 외국인들도 그 열기에 놀라움을 금치 못했습니다.\n",
            "\n",
            "셋째, 한국 대표팀의 성적이 역대 최고였기 때문에 많은 이목을 받았습니다. 한국은 16강에서 이탈리아와의 경기에서 결승골을 넣어 2-1로 승리하고, 대회 역사상 처음으로 4강에 진출했습니다. 그러나 준결승에서 터키에 패해 3등전에 진출하지 못했지만, 이는 대회 역사상 성적이 가장 좋은 성적이었습니다.\n",
            "\n",
            "이러한 이유들로 인해 2002년 월드컵에서 한국은 가장 화제가 되었던 나라로 평가받았습니다. 그들의 열정과 성과는 축구 팬들에게 오랜 기억으로 남아 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 코드로 형식 지정됨\n",
        "import openai\n",
        "\n",
        "openai.api_key=''  # openAPI API key\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[{\"role\": \"user\", \"content\": \"인공지능에 대해 알려줘\"}],\n",
        "  temperature=1,\n",
        "  top_p=1,\n",
        "  presence_penalty=1,\n",
        "  frequency_penalty=1,\n",
        "  n=1,\n",
        "  max_tokens=500,\n",
        "  stop=None\n",
        ")\n",
        "\n",
        "# print(response)\n",
        "print(response['choices'][0]['message']['content'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaklTJDcD0cR",
        "outputId": "54f5f707-e9f7-442d-949d-5ecbf1b01a77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인공지능은 인간과 유사한 지능을 갖는 컴퓨터 시스템입니다. 인공지능은 주어진 문제를 해결하거나 인간처럼 판단하고 의사 결정할 수 있는 능력을 가집니다.\n",
            "\n",
            "인공지능은 다양한 분야에서 사용됩니다. 예를 들면, 음성인식 기술로 구글 어시스턴트나 애플의 Siri와 같은 개인 비서가 있습니다. 사진 및 영상 인식 기술을 통해 얼굴과 물체를 식별하는데 사용되기도 합니다. 자율주행 자동차 등 교통 분야에서도 활용되며, 금융 서비스, 보안 시스템, 의료 진단 등에도 적용될 수 있습니다.\n",
            "\n",
            "인공지능은 크게 약한 인공지능과 강한 인공지능으로 나뉩니다. \n",
            "\n",
            "1) 약한 AI: 특정 문제를 해결하기 위해 설계된 제한된 범위 내에서 작동합니다. 예를 들어 체스 게임이나 바둑 대결 등 특정 게임에서의 컴퓨터 상대가 이에 해당합니다.\n",
            "\n",
            "2) 강한 AI: 일반적으로 사람보다 뛰어난 판단력과 학습 능력을 갖추었다고 말할 수 있는 인공지능입니다. 이는 다양한 도메인에서 인간과 유사한 활동을 수행하며, 추상적인 문제를 해결하는 능력도 갖출 수 있습니다.\n",
            "\n",
            "이러한 기술의 발전에 따라 AI 기반의 서비스와 애플리케이션들이 점점 더 활용되고\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key=''\n",
        "\n",
        "prompt = 'sklearn에서 제공해주는 titanic 데이터셋을 이용해서 남여 생존을 분석하는 프로그램을 파이썬으로 구현해줘'\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "  max_tokens=500,\n",
        "  stop=None\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8EZQnU0F4Sh",
        "outputId": "44cade24-8d60-47ae-b5ac-7b240e07c804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, 아래는 titanic 데이터셋을 사용하여 남녀 생존률을 분석하는 파이썬 프로그램입니다.\n",
            "\n",
            "```python\n",
            "import pandas as pd\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.tree import DecisionTreeClassifier\n",
            "from sklearn.metrics import accuracy_score\n",
            "\n",
            "# 데이터셋 불러오기\n",
            "data = pd.read_csv('titanic.csv')\n",
            "\n",
            "# 필요한 열 선택\n",
            "features = ['Sex', 'Age', 'Fare', 'Embarked', 'Survived']\n",
            "data = data[features]\n",
            "\n",
            "# 범주형 변수 인코딩\n",
            "data['Sex'] = data['Sex'].map({'female': 0, 'male': 1})\n",
            "data['Embarked'] = data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
            "\n",
            "# 결측치 처리\n",
            "data = data.dropna()\n",
            "\n",
            "# 독립 변수와 종속 변수 분리\n",
            "X = data.drop('Survived', axis=1)\n",
            "y = data['Survived']\n",
            "\n",
            "# 학습 데이터와 테스트 데이터로 분할\n",
            "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
            "\n",
            "# 의사결정트리 모델 학습\n",
            "model = DecisionTreeClassifier()\n",
            "model.fit(X_train, y_train)\n",
            "\n",
            "# 예측 수행\n",
            "y_pred = model.predict(X_test)\n",
            "\n",
            "# 정확도 평가\n",
            "accuracy = accuracy_score(y_test, y_pred)\n",
            "print(\"Accuracy:\", accuracy)\n",
            "```\n",
            "\n",
            "위의 코드를 실행하면, titanic.csv 파일을 불러오고 필요한 열을 선택한 뒤, 범주형 변수를 인코딩하여 데이터를 전처리합니다. 그 후, 독립 변수와 종속 변수를 분리하고 학습 데이터와 테스트 데이터로 분할합니다. 이후, 의사결정트리 모델을 학습시키고 테스트 데이터를 사용하여 예측을 수행합니다. 마지막으로, 예측 결과의 정확도를 출력\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 데이터셋 불러오기\n",
        "data = pd.read_csv('titanic.csv')\n",
        "\n",
        "# 필요한 열 선택\n",
        "features = ['Sex', 'Age', 'Fare', 'Embarked', 'Survived']\n",
        "data = data[features]\n",
        "\n",
        "# 범주형 변수 인코딩\n",
        "data['Sex'] = data['Sex'].map({'female': 0, 'male': 1})\n",
        "data['Embarked'] = data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# 결측치 처리\n",
        "data = data.dropna()\n",
        "\n",
        "# 독립 변수와 종속 변수 분리\n",
        "X = data.drop('Survived', axis=1)\n",
        "y = data['Survived']\n",
        "\n",
        "# 학습 데이터와 테스트 데이터로 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 의사결정트리 모델 학습\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 예측 수행\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 정확도 평가\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "DhKICK9GHxXJ",
        "outputId": "d5952a51-a006-44cb-e21b-4ac9e5e443ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-5aa3f740a990>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 데이터셋 불러오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'titanic.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 필요한 열 선택\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanic.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "자바와 오라클을 연동해서 오라클의 employees의 데이터를 자바에서 가져와서 출력해주는 프로그램을 구현해줘"
      ],
      "metadata": {
        "id": "terpNYN6I4rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key=''\n",
        "\n",
        "prompt = 'data변수에 저장된 문자열의 길이를 구하는 프로그램을 자바로 후원해줘'\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "  max_tokens=500,\n",
        "  stop=None\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP9lG7tDIQJ2",
        "outputId": "851fc0f6-2bec-4ce4-8748-127225f6ca2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "아래는 자바로 data 변수에 저장된 문자열의 길이를 구하는 코드입니다.\n",
            "\n",
            "```java\n",
            "public class CalculateStringLength {\n",
            "    public static void main(String[] args) {\n",
            "        String data = \"Hello, World!\";\n",
            "        int length = data.length();\n",
            "        System.out.println(\"문자열의 길이: \" + length);\n",
            "    }\n",
            "}\n",
            "```\n",
            "\n",
            "위 코드에서 `data.length()`를 호출하여 문자열의 길이를 구한 후, 결과를 `length` 변수에 저장합니다. 마지막으로 `length` 값을 출력하면 문자열의 길이가 출력됩니다.\n",
            "\n",
            "만약 다른 문자열을 사용하고 싶다면, `data` 변수에 새로운 문자열을 할당하면 됩니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key=''\n",
        "\n",
        "prompt = '자바와 오라클을 연동해서 오라클의 employees의 데이터를 자바에서 가져와서 출력해주는 프로그램을 구현해줘'\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "  max_tokens=500,\n",
        "  stop=None\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "id": "TVamoFl5JhsY",
        "outputId": "3557cabf-be0f-4acd-ec95-99325621f92f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "아래는 자바와 오라클을 연동하여 오라클의 employees 데이터를 가져와서 출력하는 예제입니다.\n",
            "\n",
            "```java\n",
            "import java.sql.*;\n",
            "\n",
            "public class OracleConnection {\n",
            "    public static void main(String[] args) {\n",
            "        try {\n",
            "            // 오라클 드라이버를 로드합니다.\n",
            "            Class.forName(\"oracle.jdbc.driver.OracleDriver\");\n",
            "\n",
            "            // 오라클 데이터베이스와 연결합니다.\n",
            "            Connection connection = DriverManager.getConnection(\"jdbc:oracle:thin:@localhost:1521:XE\", \"사용자명\", \"비밀번호\");\n",
            "\n",
            "            // SQL 쿼리를 생성합니다.\n",
            "            String query = \"SELECT * FROM employees\";\n",
            "\n",
            "            // SQL 쿼리를 실행합니다.\n",
            "            Statement statement = connection.createStatement();\n",
            "            ResultSet resultSet = statement.executeQuery(query);\n",
            "\n",
            "            // 결과를 출력합니다.\n",
            "            while (resultSet.next()) {\n",
            "                int employeeId = resultSet.getInt(\"employee_id\");\n",
            "                String firstName = resultSet.getString(\"first_name\");\n",
            "                String lastName = resultSet.getString(\"last_name\");\n",
            "                Date hireDate = resultSet.getDate(\"hire_date\");\n",
            "\n",
            "                System.out.println(\"Employee ID: \" + employeeId);\n",
            "                System.out.println(\"First Name: \" + firstName);\n",
            "                System.out.println(\"Last Name: \" + lastName);\n",
            "                System.out.println(\"Hire Date: \" + hireDate);\n",
            "                System.out.println(\"-----------------------------\");\n",
            "            }\n",
            "\n",
            "            // 연결을 종료합니다.\n",
            "            resultSet.close();\n",
            "            statement.close();\n",
            "            connection.close();\n",
            "        } catch (ClassNotFoundException e) {\n",
            "            e.printStackTrace();\n",
            "        } catch (SQLException e) {\n",
            "            e.printStackTrace();\n",
            "        }\n",
            "    }\n",
            "}\n",
            "```\n",
            "\n",
            "위의 코드에서 \"jdbc:oracle:thin:@localhost:1521:XE\", \"사용자명\", \"비밀번호\" 부분을 해당하는 값으로 변경하셔서 실행하시면 됩니다.\n",
            "또한, 오라클 JDBC 드라이버가 설치되어 있어야 합니다.\n"
          ]
        }
      ]
    }
  ]
}